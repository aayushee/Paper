\documentclass[letterpaper,11pt]{report}
\usepackage[boxed]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{fullpage}
\usepackage{verbatim}
\usepackage{cite}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{amsmath}


%\usepackage{noReferences}
\usepackage[small]{caption}
\usepackage[leftcaption]{sidecap}
\usepackage{graphics}
\usepackage{color}

\usepackage{hyperref}

%\usepackage{natbib}

\usepackage[dvips]{graphicx}
 % define the title
\graphicspath{ {images/}}

% Paper conservation layout. Long live the trees!!
\setlength{\oddsidemargin}{-0.4mm} % 25 mm left margin
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\textwidth}{160mm}      % 25 mm right margin
\setlength{\topmargin}{-5.4mm}     % 20 mm top margin
\setlength{\headheight}{5mm}
\setlength{\headsep}{5mm}
\setlength{\footskip}{10mm}
\setlength{\textheight}{237mm}     % 20 mm bottom margin


\setlength{\parskip}{1ex}
\parindent 0in
\def\SAONE{Specific Aim 1}
\def\SATWO{Specific Aim 2}
\def\SATHREE{Specific Aim 3}
\def\SAFOUR{Specific Aim 4}

\def\title{Mytitle}
%\def\titletwo{Thesis Proposal Title Line 2}

\begin{document}



\include{mtech_thesis_cover}

\newpage

\pagestyle{empty}
\vspace*{7.1in} 
Keywords: Gazetteer, Text Mining, Information Retrieval, OCR, Spelling Correction, Historical data, Influential people detection

\newpage

\begin{center}
\section*{Certificate}\label{section:certificate}
\end{center}
%\vspace{3in}
This is to certify that the thesis titled \textbf{``Title Title Title Title Title Title Title Title Title Title Title Title"} submitted by \textbf{Student Name} for the partial fulfillment of the requirements for the degree of \emph{Master of Technology} in \emph{Computer Science \& Engineering} is a record of the bonafide work carried out by her / him under my / our guidance and supervision in the Security and Privacy group at Indraprastha Institute of Information Technology, Delhi. This work has not been submitted anywhere else for the reward of any other degree. \\ \vspace{0.5in}

\textbf{Professor Advisor Name}\\
\textbf{Indraprastha Institute of Information Technology, New Delhi}
%\doublespacing

\begin{abstract}

 Historical newspaper archives provide a wealth of information. They
are of particular interest to genealogists, historians and scholars
for People Search.

 In this thesis, we design a People Gazetteer from
the noisy OCR text of historical newspapers and identify “influential”
people from it. A People Gazetteer is a dictionary of personal names;
each entry of the gazetteer is  a tuple containing a person name and a
list of articles in which his name occurs.

To build the People Gazetteer, we first spell correct the noisy text
using an edit distance based algorithm. A novel N-gram based
evaluation algorithm is designed for measuring the performance of the
spell corrector. Next, a Named Entity Recognizer is run on the text of
each article to identify person entities and an LDA-based topic
detector to assign categories to articles. To identify influential
people in each category of the People Gazetteer, we define the notion
of an Influential Person Index (IPI) and rank based on it. Our corpus
is a sample of 14020 newspaper articles (roughly two months’ data)
obtained from “The Sun” newspaper in the Chronicling America project.

\end{abstract}

\newpage
\pagestyle{empty}


\newpage



\section*{Acknowledgments}\label{section:acknowledgments}
\pagestyle{plain}
\pagenumbering{roman}

XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX XXXXXX 

\newpage

\tableofcontents
\listoffigures 
\listoftables

\newpage

\newpage

\newpage
\mbox{}


%\doublespacing

\chapter{Introduction}\label{chapter:introduction}
%\pagestyle{fancy}
\pagenumbering{arabic}
\setcounter{page}{1}
\onehalfspacing

Aim?

Motivation? 

What is a Gazetteer?

Gate Gazetteer
The goal of this research is to enable users to find influential people in a gazetteer of historical newspaper archives. The main users  include genealogists and other library patrons so that  they can efficiently search for articles relating to people that are of interest to them across several newspaper categories on a timeline.


\chapter{Related Work}

\chapter{Data Description}\label{chapter:data description}

This chapter describes the dataset used for developing the People Gazetteer. Following sections provide details of data source, characteristics and some data statistics.

\section{Data Source} 

The dataset has been taken from Chronicling America.
\noindent \emph{Chronicling
America}\footnote{\texttt{http://chroniclingamerica.loc.gov/}} is an
initiative of the National Endowment for Humanities (NEH) and the
Library of Congress (LC) whose goal is to develop an online,
searchable database of historically significant newspapers between
1836 and 1922. The New York Public Library (NYPL) is part of this
initiative and has scanned 200,000 newspaper pages published between
1890 and 1920 from microfilm.

In order to make a newspaper available for searching on the Internet,
the following processes used in \cite{dutta2011learning} must take place: (1) the microfilm copy or
paper original is scanned; (2) master and Web image files are
generated; (3) metadata is assigned for each page to improve the
search capability of the newspaper; (4) OCR software is run over high
resolution images to create searchable full text and (5) OCR text,
images, and metadata are imported into a digital library software
program. The scanned newspaper holdings of the NYPL offers a wealth of
data and opinion for researchers and historians.

The newspaper titles and digitized pages available through the
Chronicling America website can be searched using the OpenSearch
protocol\footnote{\texttt{http://www.opensearch.org/Home}}.
Unfortunately, the current search facilities are rudimentary and
irrelevant documents are often more highly ranked than relevant ones.
The newspapers are scanned on a page-by-page basis and article level
segmentation is poor or non-existent; the OCR scanning process is far
from perfect and the documents generated from it contains a large
amount of garbled text. In a bid to serve its patrons better, the New
York Public Library employed human annotators to clean headlines of
articles and text, but the process of manually reading all the old
newspapers article-by-article and cleaning them soon became very
expensive. 

\section{Data Characteristics}
An individual OCR text article has at least one or more of the following types of spelling errors:

\begin{figure}[hbt]
\includegraphics[scale=0.75]{originalimage}
\includegraphics[scale=0.80]{ocr}
\caption{Scanned Image of a Newspaper article (left) and its OCR raw text (right)}
\end{figure}

\begin{description}
 \item[$\bullet$Real word errors] include words that are spelled correctly in the OCR text but still incorrect when compared to the original newspaper article image. For example: In Figure 3.1, the word ``coil"  has been correctly spelled in the OCR text  but should have been ``and" according to the original newspaper article. 
 \item[$\bullet$Non-real word errors] include words that have been misspelled due to some insertion, deletion, substitution or transposition of characters from a word. For eg. In Figure 3.1, the word ``tnenty" in the OCR text has a substitution error (`n' should have been `w') which is actually ``twenty" according to the original newspaper article.
 \item[$\bullet$Non-word errors] include words that have been spelled incorrectly and are a combination of alphabets and numerical characters. For example: In Figure 3.1, the word ``4anrliteii" which is a combination of alphabets and number and should have been ``confident" as per the original newspaper article.
\item[$\bullet$New Line errors] include words that are separated by hyphens where part of a word is written on one text line and remaining part in the next line. For example: In Figure 3.1, the word ``ex-ceptionally" where ``ex" occurs on one line while ``ceptionally" in the next and due to no punctuation in the text, they are treated as separate words in OCR text.
\item[$\bullet$Word Split and Join errors] include words that either get split into one of more parts or some words in a sentence get joined to a make a single word. For example: In Figure 3.1, the word ``Thernndldntesnra" in the OCR text is actually a combination of three words ``The candidates are" while the words ``v Icrory" are actually equivalent to a single word ``victory" when compared with the original news article.
\end{description} 

\section{Data Statistics}
The OCR text available from Chronicling America website is on a page by page level and no article level segmentation is provided. OCR text dataset is therefore, taken from a PostgreSQL database where article level segmentation is available for two months of articles of ``The Sun" newspaper from November-December 1894 consisting of 14020 news articles with a total of 8,403,844 tokens. The newspaper database ER diagram \footnote{https://power.ldeo.columbia.edu/twiki/pub/Incubator/BodhiDBDesign/Final ERD.pdf }
is used to extract the required articles text from the database by dumping complete dataset and extracting individual articles linetext based on their unique ID. The individual text articles generated from the database do not have any punctuation and contain a large amount of garbled text containing above mentioned OCR errors.


\chapter{Data Preprocessing}\label{chapter:data preprocessing} 

 
This chapter describes the preprocessing steps applied on the news articles dataset.
The large number of OCR errors in dataset make it extremely garbled and difficult to work with. This makes data preprocessing a mandatory task in order for it to be used for any text mining task. 

\section{Spelling Correction}
 The garbled OCR dataset consists of several kinds of spelling errors and needs to be refined by correcting the text with the help of a human editor manually or automatic spelling corrector. Due to the huge  size of dataset, human editing would be extremely time consuming and expensive making it impossible and indicating requirement of a spelling correction technique. 
The main aim of the research being finding influential people, the spelling correction of person named entities also requires due consideration so that the person names entities with correct spellings get detected as a result.
The following sections describe a brief review of Spelling correction algorithms in general, the spelling correction algorithm used in this research as well as its testing and evaluation results on the OCR dataset.

\subsection{Related Work}

Kukich\cite{kukich1992techniques} comprehensively discusses various spelling correction techniques based on Non word, Isolated word and Real word spelling errors. N-gram analysis, Dictionary lookup and Probabilistic techniques are used for correcting isolated and nonword errors while Context-Dependent techniques are used mostly for correcting real word errors including the correction of word split and join errors \cite{elmi1998spelling}.

 N-gram techniques work by examining each n-gram in the text string and comparing against a pre-compiled table of n-gram statistics to retrieve the correct word while Dictionary look up techniques directly check whether the text string appears in the dictionary using string matching algorithms. Both techniques require a dictionary or a large text corpus and take frequency of n-grams or word occurrence into account in order to find the correct spelling \cite{strohmaier2003lexical}, \cite{ringlstetter2007text}.
 Probabilistic techniques use transition and word confusion probabilities to estimate likelihood of the correction in order to rank and retrieve correct word spelling.

On the other hand, Context-dependent techniques require contextual information and use either extensive NLP techniques or Statistical Language Modeling (SLM) for spelling correction.
Bassil and Alwani\cite{bassil2012ocr} use Google 1-5 gram word dataset to gain context information in order to determine the correct words sequence in the text for correction.
Tong and Evans\cite{tong1996statistical} use SLM approach involving information from letter n-grams,character confusion and word bi-gram probabilities to perform context sensitive spelling correction obtaining a 60 percent error reduction rate. 

All these spelling correction techniques have developed over time and have been used in combination to achieve improved accuracy \cite{brill2000improved}. \cite{agarwal2013utilizing} use a combination of Google suggestions, LCS and character confusion probabilities for choosing the correct spelling on a small set of historical newspaper data and achieve recall and precision of 51% and 100% respectively.


Edit distance approach, suggested initially by Wagner and Fischer\cite{wagner1974string}, is a dictionary lookup approach commonly used for OCR data correction because of the large number of substitution errors in OCR data  \cite{kukich1992techniques}\cite{christen2006comparison} which can be corrected using this technique. String edit distance approaches with faster correction are discussed in \cite{marzal1993computation},\cite{schulz2002fast}  with variants like Levenshtein automata and normalized edit distance.

Personal name spelling correction has also been studied separately including comparison among various techniques indicating that there is no one single technique that outperforms all others though pattern matching techniques results in better matching quality compared to phonetic encoding techniques\cite{christen2006comparison}. Almost all the personal name spelling correction techniques use personal names directory/dictionary for matching against wrongly spelled names in the dataset or queries in case of People Search.\cite{udupa2010hashing} 


There has not been much related work regarding automatic evaluation of word-by-word post spelling correction on OCR dataset consisting of Word Split and Join errors. Semi-automatic spelling correction system is discussed in \cite{taghva2001ocrspell} that corrects these errors but requires user interaction in order to perform complete correction and system evaluation. Rice\cite{rice1996measuring} discusses OCR errors similar to the ones in our dataset in detail with evaluation of edit distance spelling correction by calculating Word accuracy in terms of percentage of correctly recognized words by calculating length of LCS corrected between correct and incorrect strings on a page by page level. The evaluation strategy works correctly but the definition of accuracy does not give a complete coverage of the spell correction as it does not provide any information on the errors missed by the spelling corrector.  
 
 We use edit distance algorithm for spelling correction because of its speed and ability to correct OCR errors compared to the n-gram approach \cite{chattopadhyaya2013fast}. Context-dependent spelling correction is not used because of unavailability of n-gram words corpus or ground truth dataset containing OCR and true word pairs. 
A novel algorithm based on N-gram approach is then proposed for automatic evaluation of the corrected text word-by-word against the manually corrected subset of the news articles dataset. 

\subsection{Spelling Correction Algorithm}

The Edit Distance algorithm based on Levenshtein distance\cite{levenshtein1966binary} has been used for spelling correction. It is an isolated word correction technique that uses dictionary based-look up method and distance between strings for matching the text and correcting it. An ``edit distance" corresponds to the minimum number of insertions, deletions, and substitutions required to transform one string into another. The algorithm corrects Non-Real Word spelling errors up to an edit distance of 2 , i.e. , it corrects words which have spelling errors that can be corrected by making at most 2 operations of insertion, deletion and substitution of letters in the word.
The spelling corrector has been designed as suggested by Peter Norvig \footnote{ http://norvig.com/spell-correct.html}. The algorithm requires a dictionary which is used to check if each word of the text exists in it or not. If the word already exists in the dictionary then no change is made to the word and if not, then a candidate list of words is created from the word to be corrected by inserting, substituting or deleting up to 2 letters from it. This list of words is again checked for in the dictionary and returned as suggestions for the word to be corrected. The correction is made with the word formed from lowest edit distance and occurring with more frequency in the dictionary. This makes the edit distance algorithm dependent on the type of dictionary chosen for correction which means the dictionary must be well chosen for spelling correction of a specific document collection. The algorithm runs faster by reading the dictionary only once and keeping a data structure in memory for its word counts which can be referred to whenever a word comes up for correction.

\subsection{Spelling Correction Algorithm Evaluation}
Following sections describe the evaluation parameters for estimating the performance of Spelling Corrector on the OCR dataset:

\subsubsection{Accuracy}

To test the performance of this algorithm, the raw OCR text and OCR text after application of spelling correction algorithm needs to be compared with the original newspaper text. The OCR text is extremely garbled with Word Split and Join errors due to which word-to-word alignment with the original newspaper text is impossible. For this purpose, a testing algorithm has been developed for evaluation. The evaluation metric used for measuring the performance of Spelling correction algorithm is Accuracy which requires calculation of how many OCR errors got corrected when compared to the original scanned newspaper text. The measure has been chosen so as to include the complete text coverage and not just check for words that got corrected after spell correction as in the latter case, the  number of FP and TN get missed which won't give the correct measure of how well the spell corrector works. The formula used for calculating Accuracy is defined by Manning and Schutze,1999 (p.268-269) as follows:

$Accuracy=  \dfrac{TP+FP} {TP+ FP + TN + FN}$


where 

TP=Number of True Positives,

TN=Number of True Negatives,

 FP=Number of False Positives,

 FN=Number of False Negatives. 

The aim of the testing algorithm is to make a word-to-word correspondence between the OCR corrected text and the original OCR text and to mark each token in the OCR text as a TP, FP, TN or FN. Reynaert and Martin\cite{reynaert2008all} suggest a way to define these terms by distinguishing between correct words and incorrect words in the text through the set of non-target, target and selected words and use Precision and Recall evaluation measures for measuring performance of spelling correction. 

According to our spelling corrector, a ``true positive" is said to occur when a word from the OCR text gets corrected and the corrected spelling matches the one in original article text while a ``false positive" occurs if the corrected spelling does not match the corresponding word in the original article text. A ``true negative" occurs when a word does not get corrected by the algorithm as it is already correct and matches the correct word in the original text also. On the other hand, a ``false negative" occurs when the algorithm is unable to correct the word (there is no change in spelling of the word) and it does not match the corresponding word in the original text but should have been corrected.

To make this correspondence, a window of n-word grams in the scanned image text article is considered (Original.txt). For each token in the spell corrected text (Corrected.txt), the corresponding token  in the scanned text article along with 2 tokens before and 2 tokens after it are considered for alignment. If the token being considered matches with any of these words in the scanned text article words window and its spelling has been corrected when compared to the corresponding token in raw OCR text (OCR.txt), then it is marked as a ``True Positive" which is actually rewarding the Spell corrector for making the correct spelling change. A ``False Positive" is marked if it does not match any of the words despite its spelling being corrected. If the token being considered matches any of the words in the words window but no spelling correction has been made for it, then it is marked as a ``True Negative" and if it does not match any word in the window and the spelling corrector also did not correct it, then it is marked as a ``False Negative" as the word got missed by the corrector. 

Several cases could occur like difference in the lengths of linetext between OCR and Original text or while considering the first, second or the last tokens from the Corrected text for which the corresponding word window in Original text needs to be smaller. All such cases have been covered in the algorithm which calls function `MatchWordGrams' for these different cases. 

A limitation of this evaluation algorithm is that it requires all 3 versions of a newspaper article (Original, Corrected and OCR) to have the same number of lines. In case of difference in the number of lines of text due to some Word Break and Join errors, the words window needs to be extended so as to cover previous and next line texts also for alignment.


\begin{algorithm}[!h]
\caption{Evaluation Algorithm for Spell Correction}
  \KwIn{Ocr.txt,Corrected.txt,Original.txt}
  \KwOut{Spell Corrector Accuracy }
\SetKwFunction{MatchWordGrams}{MatchWordGrams}%
 $OcrLine$:=a line of text from Ocr.txt\;
 $CorrectedLine$:=a line of text from Corrected.txt\;
 $OriginalLine$:=a line of text from Original.txt\;
 $tp \leftarrow $0  $fp \leftarrow $0 $tn \leftarrow $0 $fn\leftarrow $0\;  
	\For{(int i=0; i $<$ CorrectedLine.length ; i++) }
	{

    \If{(CorrectedLine.length$<$4 $||$ OriginalLine.length$<$4)}
	{		
    	\MatchWordGrams{(OcrLine,CorrectedLine,OriginalLine,0,OriginalLine.length,i)}\; 
	}
    \Else{
   \If {(i==0)}
   {
\MatchWordGrams{(OcrLine,CorrectedLine,OriginalLine,0,3,0)}\;
   }
   \ElseIf{ (i==1)}
   {
\MatchWordGrams{(OcrLine,CorrectedLine,OriginalLine,0,4,1)}\;
   }
	\ElseIf{(i==(CorrectedLine.length-2) $||$ (CorrectedLine.length-1) $||$ (CorrectedLine.length) $||$ (CorrectedLine.length+1))}
	{
\MatchWordGrams{(OcrLine,CorrectedLine,OriginalLine,i-2,OriginalLine.length,i)}\;
	}  
	\ElseIf{(i $>$= CorrectedLine.length+2)}
	{	
\MatchWordGrams{(OcrLine,CorrectedLine,OriginalLine,OriginalLine.length-3,OriginalLine.length,i)}\;	
	}
	\Else
	 {
\MatchWordGrams{(OcrLine,CorrectedLine,OriginalLine,i-2,i+2,i)}\;	
	}	
   }
 }
 	 $Accuracy=(tp+tn)/(tp+tn+fp+fn);$\
\end{algorithm}


\begin{algorithm}[!htb]
\caption{MatchWordGrams Function called by Algorithm 1}
\begin{algorithmic}
\Function {MatchWordGrams}{OcrLine, CorrectedLine, OriginalLine, jstart, jend, i}
  
 \For{(int j=jstart; j$<$jend; j++)}
  {
    \If{ ((CorrectedLine[i].equals(OriginalLine[j]))\&\&(!(OcrLine[i].equals(CorrectedLine[i]))))}
     {
	  $tp=tp+1$\;
	  flag0=false\;
	 \Return $tp$\;
	  }
	\ElseIf{((CorrectedLine[i].equals(OriginalLine[j]))\&\&(OcrLine[i].equals(CorrectedLine[i])))}
	      {
		 $tn=tn+1$\;
		  flag1=false\;
		\Return $tn$\;
	      }
}

	 \If{(!(OcrLine[i].equals(CorrectedLine[i]))\&\&flag0==true)}
	 {
		    $fp=fp+1$\;
		   \Return $fp$\;
            }
	 
	 \ElseIf{((OcrLine[i].equals(CorrectedLine[i])) \&\& flag1==true)}
	 {
		    $fn=fn+1$\;
		   \Return $fn$\;
	 }
\EndFunction
\end{algorithmic}
\end{algorithm}




\begin{figure} [!htb]
\includegraphics[scale=0.4]{img3}
\includegraphics[scale=0.75]{originalimg3}
\caption{Scanned image of a newspape article (left) along with its original text (right)}
\end{figure}
 
\begin{figure}[h]
\includegraphics[scale=0.75]{ocr3}
\includegraphics[scale=0.75]{corrected3}
\caption{OCR raw text (left) and Spell corrected text (right) of the article}
\end{figure}
\newpage Working of the above algorithm can be demonstrated with the help of the following example:
Consider 3 versions of a scanned image of a newspaper article,  the original text of the scanned image in Figure 3.2 and the raw OCR text and the corrected text (after spell correction) in Figure 3.3. As highlighted in the figures, for line 6 the line texts are:

\newpage OcrLine= \textit{by tltn rejmrt of th cepert aocountauts who}

CorrectedLine= \textit{by than report of the expert accountants who}

OriginalLine= \textit{by the report of the expert accountants who} 

Here, for each token of CorrectedLine, we find its index and call the MatchWordGrams function accordingly. For the first token 'by' at index i=0 in CorrectedLine, we consider the word window to be "by the report" (index j=0 to 2) in OriginalLine by matching iteratively with each token to see if there is a match and also if there has been a spelling correction by comparing with the corresponding token in OcrLine. Here, no change was made to the spelling of 'by' and it matches with a word in words window, so it is marked as a FN. For the second token 'than' at index i=1, we consider the word window to be "by the report of" (index j=0 to 3) for which there is no match in the window but there has been a spelling correction from 'tltn' to 'than', which implies the correction was wrong and the token is marked as a FP. For the third token 'report' at index i=2, we consider the window as "by the report of the" (index j=0 to 4) in Original Line and find that there is a match in the word window and there has been a spelling correction too from 'rejmrt' to 'report' which makes this token a TP. Similarly, rest of the tokens get marked for each line in the Corrected.txt. 

Another example can be considered from Line 10 in Figure 3.2 and Figure 3.3 where the number of tokens is different in CorrectedLine and OriginalLine. In such a case, direct alignment between tokens is not possible because of which the words window becomes useful. Here, when the last token 'Richmond' of CorrectedLine is considered at index i=3, the corresponding words window becomes "Jury now sitting at Richmond" (index j=1 to 5) for which there is a match in the words windows and corresponding spelling has also been changed from 'tilchmond' to 'Richmond' which makes it a TP. Had the word window not been considered, the corresponding token at index j=3 in OriginalLine would have been chosen as 'sitting' which would have resulted in a FP. 
   

\subsubsection{Time taken for Spelling Correction}
Time is also an essential parameter while measuring the performance of spelling correction. Since the dataset is quite large, it is important that the algorithm does not take too long to correct an article and can be parallelized in case it takes more time for correction.

\subsubsection{Person Names Detection Rate}
The spelling correction algorithm is evaluated on the basis of another parameter which is used to consider the special case of person entity names spelling correction as the main goal of research is to detect these names with correct spellings.
Person Names Detection Rate can be defined as the ratio of person names recognized through Named Entity Recognition before spelling correction process and the total number of person names recognized in the original newspaper articles.

$Person Names Detection Rate=\dfrac{ \text{Person Names recognized before/after spelling correction}} {\text{ Person Names recognized in original newspaper articles}} $


\subsection{Spelling Correction Algorithm Evaluation Results}
The spelling correction algorithm is used to correct all the 14020 OCR raw text articles in the dataset. The dictionary used for look-up is a concatenation of several public domain books from Project Gutenberg and lists of most frequent words from Wiktionary and the British National Corpus \footnote{http://norvig.com/big.txt} along with a large people names list which is obtained  by running Stanford NER-CRF parser on dataset from TREC 2013 Crowdsourcing track. \footnote{http://boston.lti.cs.cmu.edu/clueweb12/TRECcrowdsourcing2013/}
The spelling corrector is quite fast and takes 9 seconds on an average to correct the newspaper OCR articles. It takes a total of 36 hours to run on 14020 articles.
Though the algorithm does not require parallelizing here but it can be parallelized by getting candidate lists for insertion, deletion, substitution and transposition of a word through parallel threads and combining them together for correction of a word.
 
For evaluation of the spelling corrector, 3 versions of each newspaper article are required: OCR raw text, spelling corrector corrected text and the original scanned newspaper article text. Since the dataset is quite large (14020) and it is not possible to get original text of each of these newspaper images, a smaller number of articles are chosen to study the results of spelling correction. 50 scanned newspaper images are taken and an online OCR \footnote{www.onlineocr.net} is run on them followed by some manual correction to get the original articles text. 
Accuracy can then be calculated for all 3 versions of 50 newspaper articles using the Evaluation algorithm by marking each word in the OCR text article as a TP, FP, TN or FN. 

The spelling corrector shows an Accuracy of 72.7 \%  when corrected text is compared to OCR text and original article text. The results are less accurate due to a large number of Non-word, New Line, Word Join and Break errors in the OCR data which can not be corrected by the spelling correction algorithm.
But the spelling correction is useful in terms of PersonNamesDetectionRate \textit{(PNDR)}. Following are the statistics obtained for \textit{PNDR}:

\textit{PNDR} for Raw OCR Text: 72.5\% 

\textit{PNDR} for Spell Corrected Text without Person names correction: 63.3\% 

\textit{PNDR} for Spell Corrected Text after: 82.5\% 

These statistics indicate that spelling correction using an extended dictionary for personal names is useful for detecting person names from the garbled newspaper articles and the results are dependent on the type of dictionary being used for spell correction.
 
A better result can be obtained by correcting the New Line errors in the articles. This can be done by checking for if the word at last index of a text line or the word at first index of the next text line is a word not present in the dictionary and combining the two and checking again in the dictionary for a valid word. The new word, if present in the dictionary can be replaced by the two words from which it is formed thereby removing the New Line error. Similar approach can be applied for Word Break and Join errors but would require each word of an article not present in the dictionary to be analyzed along with some window of words before and after it to make a correction. 
 
\chapter{PEOPLE GAZETTEER}\label{chapter:people gazetteer}

This chapter describes the process of construction of people gazetter by extracting person names from the news articles dataset (NER) followed by formation of chains of articles for each person entity over different article topics using LDA topic detection.

Dataset statistics

Output of people gazetteer

\chapter{Influential People Detection}\label{chapter:influential people detection}
\chapter{Results and Conclusion}\label{chapter:results and conclusion}
%\newpage
%\bibliographystyle{these}
\bibliographystyle{acm}
%\bibliographystyle{elsart-harv}
%\newpage
%\section{References}
%\bibliography{Library}

\bibliography{aayushee}
\chapter*{Appendix}\label{chapter:appendix} 

\end{document}
